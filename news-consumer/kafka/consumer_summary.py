import os
import json
import time
import openai
import hashlib
import threading
import sys
from pymongo import MongoClient
from confluent_kafka import Consumer
from dateutil import parser
from datetime import datetime
import asyncio

# Datadog tracing
from ddtrace import tracer

# API ì„œë²„ ëª¨ë“ˆ import
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from api_server import stats, start_api_server

from utils import build_prompt, call_openai, save_summary_to_mongo
from mysql_utils import create_kafka_test_table, save_test_message_to_mysql

def run_summary_consumer():
    print("ğŸš€ News Consumer Summary Service Starting... (GitHub Actions Test - v2024.1.20)")
    
    # í†µê³„ ìƒíƒœ ì—…ë°ì´íŠ¸
    stats.set_consumer_status(True)
    
    # MySQL í…ŒìŠ¤íŠ¸ í…Œì´ë¸” ì´ˆê¸°í™”
    print("ğŸ“Š Initializing MySQL test table...")
    create_kafka_test_table()
    
    openai.api_key = os.getenv("OPENAI_API_KEY")
    mongo = MongoClient(os.getenv("MONGO_URI", "mongodb://localhost:27017"))
    collection = mongo["bitsfeed"]["news"]

    consumer = Consumer({
        'bootstrap.servers': os.getenv("KAFKA_BOOTSTRAP_SERVERS", "localhost:9092"),
        'group.id': 'summary-consumer',
        'client.id': 'news-consumer-client',  # Data Streams Monitoringì„ ìœ„í•œ í´ë¼ì´ì–¸íŠ¸ ì‹ë³„
        'auto.offset.reset': 'earliest',
        'enable.auto.commit': True,
        'session.timeout.ms': 10000,          # Consumer Group ì„¸ì…˜ íƒ€ì„ì•„ì›ƒ
        'heartbeat.interval.ms': 3000,        # Consumer Group heartbeat
        'max.poll.interval.ms': 300000,       # ìµœëŒ€ poll ê°„ê²©
        'statistics.interval.ms': 10000,      # í†µê³„ ì •ë³´ ìˆ˜ì§‘ (Data Streams ê´€ë ¨)
    })
    print("news_raw topic subscribe")
    consumer.subscribe(['news_raw'])
    stats.set_kafka_status("connecting")

    async def poll_loop():
        print("poll_loop started")
        
        # Consumer Group coordinatorì™€ partition í• ë‹¹ ëŒ€ê¸°
        print("ğŸ”„ Waiting for partition assignment...")
        assignment_retries = 0
        max_retries = 30  # ìµœëŒ€ 30ì´ˆ ëŒ€ê¸°
        
        while not consumer.assignment() and assignment_retries < max_retries:
            consumer.poll(0.1)
            await asyncio.sleep(0.1)
            assignment_retries += 1
            if assignment_retries % 10 == 0:  # 1ì´ˆë§ˆë‹¤ ë¡œê·¸ ì¶œë ¥
                print(f"ğŸ”„ Still waiting for partition assignment... ({assignment_retries}/30)")
        
        final_assignment = consumer.assignment()
        if final_assignment:
            print("âœ… Kafka partition assigned:", final_assignment)
            # Data Streams Monitoringì„ ìœ„í•œ ì¶”ê°€ ì •ë³´ ë¡œê·¸
            for tp in final_assignment:
                print(f"ğŸ“Š DSM: Topic={tp.topic}, Partition={tp.partition}, Consumer Group=summary-consumer")
        else:
            print("âŒ Failed to get partition assignment after 30 seconds")
            
        consumer.list_topics(timeout=5.0)
        stats.set_kafka_status("connected")

        try:
            while True:
                msg = consumer.poll(1.0)
                if msg is None:
                    await asyncio.sleep(0.1)
                    continue

                if msg.error():
                    print(f"âŒ Kafka error: {msg.error()}")
                    stats.update_error()
                    stats.set_kafka_status("error")
                    continue

                try:                    
                    article = json.loads(msg.value().decode('utf-8'))
                    print("Message í™•ì¸: ", article)
                    
                    # Datadog spanìœ¼ë¡œ message ì²˜ë¦¬ ì¶”ì 
                    with tracer.trace("kafka.message.process", service="news-consumer") as span:
                        span.set_tag("kafka.topic", "news_raw")
                        span.set_tag("kafka.partition", msg.partition())
                        span.set_tag("kafka.offset", msg.offset())
                        span.set_tag("message.id", article.get('_id', 'unknown'))
                        
                        # í…ŒìŠ¤íŠ¸ ë©”ì‹œì§€ ê°ì§€ ë° MySQL ì €ì¥
                        if 'test_metadata' in article and article.get('test_metadata', {}).get('test_type') == 'data_streams_monitoring':
                            span.set_tag("message.type", "test")
                            span.set_tag("test.type", "data_streams_monitoring")
                            print("ğŸ§ª Test message detected for Data Streams Monitoring")
                            
                            with tracer.trace("mysql.test_message.save", service="news-consumer") as mysql_span:
                                mysql_span.set_tag("message.title", article.get('title', 'unknown'))
                                if save_test_message_to_mysql(article):
                                    print("âœ… Test message saved to MySQL successfully")
                                    mysql_span.set_tag("save.status", "success")
                                else:
                                    print("âŒ Failed to save test message to MySQL")
                                    mysql_span.set_tag("save.status", "failed")
                            
                            # í†µê³„ ì—…ë°ì´íŠ¸ (í…ŒìŠ¤íŠ¸ ë©”ì‹œì§€ë„ ì²˜ë¦¬ ì¹´ìš´íŠ¸ì— í¬í•¨)
                            stats.update_processed(article, {"summary_ko": "Test message processed", "impact_ko": "Monitoring test"})
                            print(f"âœ… í…ŒìŠ¤íŠ¸ ë©”ì‹œì§€ ì²˜ë¦¬ ì™„ë£Œ - ì´ {stats.total_processed}ê°œ ì²˜ë¦¬ë¨")
                        else:
                            # ì¼ë°˜ ë‰´ìŠ¤ ë©”ì‹œì§€ ì²˜ë¦¬
                            span.set_tag("message.type", "news")
                            article_id = article.get('_id') or hashlib.md5(article['link'].encode()).hexdigest()
                            
                            with tracer.trace("openai.summary.generate", service="news-consumer") as ai_span:
                                ai_span.set_tag("article.id", article_id)
                                ai_span.set_tag("article.title", article.get('title', 'unknown'))
                                prompt = build_prompt(article)
                                summary = call_openai(prompt)
                            
                            with tracer.trace("mongo.summary.save", service="news-consumer") as mongo_span:
                                mongo_span.set_tag("article.id", article_id)
                                save_summary_to_mongo(collection, article_id, summary)
                            
                            # í†µê³„ ì—…ë°ì´íŠ¸
                            stats.update_processed(article, summary)
                            print(f"âœ… ì¼ë°˜ ë‰´ìŠ¤ ì²˜ë¦¬ ì™„ë£Œ - ì´ {stats.total_processed}ê°œ ì²˜ë¦¬ë¨")
                    
                except Exception as e:
                    print(f"â— ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                    stats.update_error()

        except asyncio.CancelledError:
            print("ğŸ›‘ Summary consumer shutdown requested.")
        finally:
            stats.set_consumer_status(False)
            stats.set_kafka_status("disconnected")
            consumer.close()
            print("ğŸ›‘ Kafka consumer closed.")

    asyncio.run(poll_loop())

def start_service():
    """API ì„œë²„ì™€ Kafka Consumerë¥¼ ë™ì‹œ ì‹¤í–‰"""
    print("ğŸ“Š Initializing News Consumer Summary Service...")
    
    # API ì„œë²„ë¥¼ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹œì‘
    api_thread = threading.Thread(target=start_api_server, daemon=True)
    api_thread.start()
    print("ğŸŒ API Server starting on port 8002...")
    
    # ì ì‹œ ëŒ€ê¸° (API ì„œë²„ ì‹œì‘ ì‹œê°„)
    time.sleep(2)
    
    # Kafka Consumer ì‹œì‘
    print("ğŸ”§ Starting Kafka Consumer...")
    run_summary_consumer()

if __name__ == "__main__":
    start_service()
