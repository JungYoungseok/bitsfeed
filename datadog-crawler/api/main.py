from fastapi import FastAPI
import asyncio
from crawler.google_news import fetch_google_news
from db.mongodb import insert_news, get_all_news
from fastapi.middleware.cors import CORSMiddleware
from crawler.scheduler import start_scheduler, crawl_all_news
from api.test_consume import router as test_consume_router
from crawler.rss import fetch_datadog_rss

try:
    from fastapi import FastAPI
    app = FastAPI(
        title="Datadog News Crawler",
        description="ë‰´ìŠ¤ í¬ë¡¤ë§ ë° ìˆ˜ì§‘ ì „ìš© ì„œë¹„ìŠ¤",
        version="1.0.0"
    )
except Exception as e:
    print("âŒ FastAPI ë¡œë”© ì‹¤íŒ¨:", e)
    raise e

# ê¸€ë¡œë²Œ ìŠ¤ì¼€ì¤„ëŸ¬ ë³€ìˆ˜
scheduler = None

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # or ["http://localhost:3000"]
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# í¬ë¡¤ë§ ê´€ë ¨ ë¼ìš°í„°ë§Œ ë“±ë¡
app.include_router(test_consume_router)

# ì•± ì‹œì‘ ì‹œ ìŠ¤ì¼€ì¤„ëŸ¬ ìë™ ì‹¤í–‰
@app.on_event("startup")
async def startup_event():
    """ì•± ì‹œì‘ ì‹œ ë‰´ìŠ¤ í¬ë¡¤ë§ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘"""
    global scheduler
    print("[STARTUP] Starting news crawler scheduler...")
    scheduler = start_scheduler()

@app.get("/")
def root():
    return {
        "service": "Datadog News Crawler",
        "version": "1.0.1", 
        "description": "ë‰´ìŠ¤ í¬ë¡¤ë§ ë° ìˆ˜ì§‘ ì „ìš© ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤",
        "build_info": "GitHub Actions Build Test ğŸš€",
        "last_updated": "2024-01-30",
        "endpoints": {
            "crawl": "/crawl",
            "news": "/news",
            "scheduler": "/scheduler/*",
            "hello": "/hello"
        },
        "analytics_service": "http://datadog-analytics:8001"
    }

@app.get("/hello")
def hello():
    return {"message": "ğŸ‘‹ Hello from Datadog News Crawler! (Lightweight & Fast)"}

@app.get("/news")
def list_news():
    return get_all_news()

@app.post("/crawl")
def crawl_news():
    """ìˆ˜ë™ìœ¼ë¡œ ë‰´ìŠ¤ í¬ë¡¤ë§ì„ ì‹¤í–‰í•˜ëŠ” ì—”ë“œí¬ì¸íŠ¸"""
    all_news = []

    google_news = fetch_google_news()
    rss_news = fetch_datadog_rss()

    all_news.extend(google_news)
    all_news.extend(rss_news)

    insert_news(all_news)

    return {"message": f"Inserted {len(all_news)} news items"}

@app.post("/crawl/immediate")
def crawl_immediate():
    """ì¦‰ì‹œ ìŠ¤ì¼€ì¤„ëœ í¬ë¡¤ë§ í•¨ìˆ˜ë¥¼ ì‹¤í–‰"""
    try:
        crawl_all_news()
        return {"message": "Immediate news crawling completed successfully"}
    except Exception as e:
        return {"error": f"Failed to run immediate crawl: {str(e)}"}

@app.get("/scheduler/status")
def get_scheduler_status():
    """ìŠ¤ì¼€ì¤„ëŸ¬ ìƒíƒœ í™•ì¸"""
    global scheduler
    if scheduler is None:
        return {"status": "not_started", "message": "Scheduler not initialized"}
    
    if scheduler.running:
        jobs = []
        for job in scheduler.get_jobs():
            jobs.append({
                "id": job.id,
                "func": job.func.__name__,
                "trigger": str(job.trigger),
                "next_run": job.next_run_time.isoformat() if job.next_run_time else None
            })
        return {
            "status": "running",
            "message": "Scheduler is active",
            "jobs": jobs
        }
    else:
        return {"status": "stopped", "message": "Scheduler is not running"}

@app.post("/kafka-test")
def start_kafka_test():
    """Datadog Data Streams Monitoring í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ Kafka Producer"""
    import threading
    import time
    import uuid
    from datetime import datetime
    from confluent_kafka import Producer
    import os
    import json
    
    def kafka_test_producer():
        """1ë¶„ ë™ì•ˆ 2-3ì´ˆë§ˆë‹¤ í…ŒìŠ¤íŠ¸ ë©”ì‹œì§€ë¥¼ ì „ì†¡í•˜ëŠ” producer"""
        producer = Producer({
            'bootstrap.servers': os.getenv("KAFKA_BOOTSTRAP_SERVERS", "localhost:9092"),
            'client.id': 'datadog-test-producer'
        })
        
        def delivery_report(err, msg):
            if err is not None:
                print(f'âŒ Message delivery failed: {err}')
            else:
                print(f'âœ… Test message delivered to {msg.topic()} [{msg.partition()}]')
        
        # 30ì´ˆ ë™ì•ˆ 3ì´ˆ ê°„ê²©ìœ¼ë¡œ 10ê°œ ë©”ì‹œì§€ ì „ì†¡
        start_time = datetime.now()
        message_count = 0
        
        try:
            while (datetime.now() - start_time).seconds < 30:
                message_count += 1
                test_message = {
                    "_id": f"test_{uuid.uuid4().hex[:8]}",
                    "title": f"ğŸ“Š Data Streams Monitoring Test Message #{message_count}",
                    "link": f"https://test.datadog.com/test-{message_count}",
                    "source": "Datadog Test Producer",
                    "published": datetime.now().isoformat(),
                    "content": f"This is a test message for Datadog Data Streams Monitoring. Message #{message_count} sent at {datetime.now().strftime('%H:%M:%S')}",
                    "test_metadata": {
                        "test_type": "data_streams_monitoring",
                        "message_id": message_count,
                        "producer": "datadog-crawler",
                        "consumer": "news-consumer",
                        "target_table": "kafka_test_logs"
                    }
                }
                
                producer.produce(
                    'news_raw',
                    key=f"test-{message_count}",
                    value=json.dumps(test_message),
                    callback=delivery_report
                )
                
                producer.poll(0)  # Trigger any available delivery report callbacks
                print(f"ğŸš€ Sent test message #{message_count} at {datetime.now().strftime('%H:%M:%S')}")
                
                time.sleep(3)  # 3ì´ˆ ëŒ€ê¸°
                
        except Exception as e:
            print(f"âŒ Kafka test producer error: {e}")
        finally:
            producer.flush()  # Wait for all messages to be delivered
            print(f"âœ… Kafka test completed. Sent {message_count} messages.")
    
    # ë°±ê·¸ë¼ìš´ë“œì—ì„œ producer ì‹¤í–‰
    producer_thread = threading.Thread(target=kafka_test_producer, daemon=True)
    producer_thread.start()
    
    return {
        "message": "ğŸš€ Kafka Data Streams Monitoring í…ŒìŠ¤íŠ¸ê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.",
        "details": {
            "duration": "30ì´ˆ",
            "interval": "3ì´ˆë§ˆë‹¤",
            "topic": "news_raw",
            "expected_messages": "ì•½ 10ê°œ",
            "producer": "datadog-crawler",
            "consumer": "news-consumer"
        },
        "monitoring": "Datadog Data Streams Monitoringì—ì„œ ì‹¤ì‹œê°„ ë°ì´í„° í”Œë¡œìš°ë¥¼ í™•ì¸í•˜ì„¸ìš”."
    }