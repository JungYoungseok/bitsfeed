from datetime import datetime
from pymongo import MongoClient, UpdateOne
import hashlib
import os
from dateutil import parser
from kafka.kafka_producer import send_news_to_kafka

MONGO_URI = os.getenv("MONGO_URI", "mongodb://mongo:27017")
client = MongoClient(MONGO_URI)
collection = client["bitsfeed"]["news"]

def serialize_news_item(item):
    """Kafka ì „ì†¡ì„ ìœ„í•œ JSON ì§ë ¬í™” ì•ˆì „ ë³µì‚¬ë³¸ ìƒì„±"""
    def encode_value(value):
        if isinstance(value, datetime):
            return value.isoformat()
        return value

    return {k: encode_value(v) for k, v in item.items()}

def update_keywords_for_article(article_id: str):
    """ë‹¨ì¼ ê¸°ì‚¬ì— ëŒ€í•œ í‚¤ì›Œë“œ ë°ì´í„° ì—…ë°ì´íŠ¸"""
    try:
        from db.keyword_manager import (
            process_article_keywords, 
            keywords_collection, 
            keyword_trends_collection
        )
        
        # ê¸°ì‚¬ ë°ì´í„° ì¡°íšŒ
        article = collection.find_one({"_id": article_id})
        if not article:
            return
        
        # í‚¤ì›Œë“œ ì¶”ì¶œ
        article_keywords = process_article_keywords(article)
        if not article_keywords:
            return
        
        # ë°œí–‰ì¼ íŒŒì‹±
        try:
            if isinstance(article.get('published'), str):
                pub_date = datetime.fromisoformat(article['published'].replace('Z', '+00:00')).date()
            else:
                pub_date = article.get('published', datetime.utcnow()).date()
        except:
            pub_date = datetime.utcnow().date()
        
        # í‚¤ì›Œë“œë³„ ë¹ˆë„ ë° ê¸°ì‚¬ ì •ë³´ ì—…ë°ì´íŠ¸
        for keyword in article_keywords:
            # keywords collection ì—…ë°ì´íŠ¸
            keywords_collection.update_one(
                {"keyword": keyword},
                {
                    "$inc": {"frequency": 1},
                    "$push": {
                        "articles": {
                            "article_id": article_id,
                            "title": article.get('title', ''),
                            "published": pub_date.isoformat(),
                            "source": article.get('source', '')
                        }
                    },
                    "$set": {"last_updated": datetime.utcnow()}
                },
                upsert=True
            )
            
            # keyword_trends collection ì—…ë°ì´íŠ¸
            keyword_trends_collection.update_one(
                {"keyword": keyword, "date": pub_date.isoformat()},
                {
                    "$inc": {"count": 1},
                    "$set": {
                        "keyword": keyword,
                        "date": pub_date.isoformat(),
                        "last_updated": datetime.utcnow()
                    }
                },
                upsert=True
            )
        
        # í‚¤ì›Œë“œ ë™ì‹œ ì¶œí˜„ ì—…ë°ì´íŠ¸
        cooccurrence_collection = client["bitsfeed"]["keyword_cooccurrence"]
        
        for i, keyword1 in enumerate(article_keywords):
            for keyword2 in article_keywords[i+1:]:
                pair_key1, pair_key2 = sorted([keyword1, keyword2])
                cooccurrence_collection.update_one(
                    {"keyword1": pair_key1, "keyword2": pair_key2},
                    {
                        "$inc": {"count": 1},
                        "$set": {
                            "keyword1": pair_key1,
                            "keyword2": pair_key2,
                            "last_updated": datetime.utcnow()
                        }
                    },
                    upsert=True
                )
        
        print(f"âœ… ê¸°ì‚¬ {article_id}ì˜ í‚¤ì›Œë“œ ë°ì´í„° ì—…ë°ì´íŠ¸ ì™„ë£Œ")
        
    except Exception as e:
        print(f"âŒ ê¸°ì‚¬ {article_id} í‚¤ì›Œë“œ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")

def insert_news(news_items):
    operations = []
    for item in news_items:
        unique_id = hashlib.md5(item["link"].encode()).hexdigest()
        item["_id"] = unique_id

        if isinstance(item["published"], str):
            item["published"] = parser.parse(item["published"])

        item["scraped_at"] = datetime.utcnow()

        operations.append(
            UpdateOne(
                {"_id": unique_id},
                {"$setOnInsert": item},
                upsert=True
            )
        )

    if operations:
        result = collection.bulk_write(operations, ordered=False)
        print(f"[MongoDB] Upserted {result.upserted_count} new items.")

        # âœ… Kafkaë¡œ ìƒˆë¡œ insertëœ ë°ì´í„°ë§Œ ì „ì†¡
        for upserted_id in result.upserted_ids.values():
            new_item = collection.find_one({"_id": upserted_id})
            send_news_to_kafka(serialize_news_item(new_item))
            
            # ğŸ”„ ìƒˆë¡œ ì¶”ê°€ëœ ê¸°ì‚¬ì˜ í‚¤ì›Œë“œ ë°ì´í„° ì—…ë°ì´íŠ¸
            update_keywords_for_article(upserted_id)

def get_all_news():
    return list(
        collection.find({}, {"_id": 0}).sort("published", -1)
    )
