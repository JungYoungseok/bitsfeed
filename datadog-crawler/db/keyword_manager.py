from datetime import datetime, timedelta
from pymongo import MongoClient, ASCENDING, DESCENDING
from pymongo.operations import UpdateOne
from collections import Counter, defaultdict
import hashlib
import os
import re
from typing import List, Dict, Optional
from db.mongodb import client

# KoNLPy í•œêµ­ì–´ ë¶„ì„ (ì„ íƒì ìœ¼ë¡œ ì‚¬ìš©)
try:
    from konlpy.tag import Okt
    okt = Okt()
    KONLPY_AVAILABLE = True
except ImportError:
    KONLPY_AVAILABLE = False
    print("âš ï¸ KoNLPyë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ í‚¤ì›Œë“œ ì¶”ì¶œì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")

# Collection ì„¤ì •
news_collection = client["bitsfeed"]["news"]
keywords_collection = client["bitsfeed"]["keywords"]
keyword_trends_collection = client["bitsfeed"]["keyword_trends"]

def create_indexes():
    """í‚¤ì›Œë“œ ê´€ë ¨ collectionë“¤ì— ì¸ë±ìŠ¤ ìƒì„±"""
    try:
        # keywords collection ì¸ë±ìŠ¤
        keywords_collection.create_index([("keyword", ASCENDING)])
        keywords_collection.create_index([("frequency", DESCENDING)])
        keywords_collection.create_index([("last_updated", DESCENDING)])
        keywords_collection.create_index([("articles", ASCENDING)])
        
        # keyword_trends collection ì¸ë±ìŠ¤
        keyword_trends_collection.create_index([("keyword", ASCENDING), ("date", ASCENDING)])
        keyword_trends_collection.create_index([("date", DESCENDING)])
        keyword_trends_collection.create_index([("keyword", ASCENDING)])
        
        print("âœ… í‚¤ì›Œë“œ collection ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")
    except Exception as e:
        print(f"âŒ ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")

def extract_keywords_simple(text: str, min_length: int = 2) -> List[str]:
    """ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ (KoNLPy ì—†ì´)"""
    if not text:
        return []
    
    # í•œê¸€, ì˜ì–´, ìˆ«ìë§Œ ì¶”ì¶œ
    text = re.sub(r'[^\w\sê°€-í£]', ' ', text)
    words = text.split()
    
    # ê¸¸ì´ í•„í„°ë§ ë° ë¶ˆìš©ì–´ ì œê±°
    stopwords = {
        'ì˜', 'ì´', 'ê·¸', 'ì €', 'ê²ƒ', 'ìˆ˜', 'ë“±', 'ë°', 'ë˜ëŠ”', 'í•˜ì§€ë§Œ', 'ê·¸ë¦¬ê³ ', 
        'ì´ëŸ°', 'ì €ëŸ°', 'ì´ê²ƒ', 'ì €ê²ƒ', 'ë“¤', 'ë¥¼', 'ì„', 'ì´', 'ê°€', 'ì€', 'ëŠ”', 
        'ì—', 'ì„œ', 'ë¡œ', 'ìœ¼ë¡œ', 'ì™€', 'ê³¼', 'ë„', 'ë§Œ', 'ë¿', 'ê¹Œì§€', 'ë¶€í„°', 
        'ì—ì„œ', 'ì—ê²Œ', 'í•œí…Œ', 'ê»˜', 'ë”', 'ê°€ì¥', 'ë§¤ìš°', 'ì•„ì£¼', 'ì •ë§', 'ë„ˆë¬´', 
        'ì¡°ê¸ˆ', 'ì•½ê°„', 'ë˜', 'ë‹¤ì‹œ', 'ë‹¤ë¥¸', 'ìƒˆë¡œìš´', 'ì˜¤ëŠ˜', 'ì–´ì œ', 'ë‚´ì¼', 
        'ì´ë²ˆ', 'ì§€ë‚œ', 'ë‹¤ìŒ', 'ìˆëŠ”', 'ì—†ëŠ”', 'ìˆë‹¤', 'ì—†ë‹¤', 'ë˜ëŠ”', 'ë˜ë‹¤',
        'í•˜ëŠ”', 'í•˜ë‹¤', 'ìˆì–´', 'ì—†ì–´', 'ëœë‹¤', 'í•œë‹¤', 'ê°™ì€', 'ë‹¤ë¥¸', 'ìˆì„',
        'ìœ„í•œ', 'ìœ„í•´', 'í†µí•´', 'ëŒ€í•œ', 'ëŒ€í•´', 'ê´€ë ¨', 'ê¸°ë°˜', 'ì¤‘ì‹¬', 'ì£¼ìš”',
        'ì „ì²´', 'ì¼ë¶€', 'ëŒ€ë¶€ë¶„', 'ëª¨ë“ ', 'ê°ê°', 'ì—¬ëŸ¬', 'ë‹¤ì–‘í•œ', 'íŠ¹íˆ'
    }
    
    filtered_words = []
    for word in words:
        if len(word) >= min_length and word not in stopwords and not word.isdigit():
            filtered_words.append(word)
    
    return filtered_words

def extract_english_keywords(text: str, min_length: int = 3) -> List[str]:
    """ì˜ì–´ í‚¤ì›Œë“œ ì¶”ì¶œ"""
    if not text:
        return []
    
    # ì˜ì–´ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
    text = re.sub(r'[^\w\s]', ' ', text.lower())
    words = text.split()
    
    # ì˜ì–´ ë¶ˆìš©ì–´
    english_stopwords = {
        'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with',
        'by', 'from', 'up', 'about', 'into', 'through', 'during', 'before', 'after', 'above',
        'below', 'between', 'among', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have',
        'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might',
        'must', 'can', 'this', 'that', 'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we',
        'they', 'them', 'their', 'there', 'where', 'when', 'why', 'how', 'what', 'which', 'who',
        'whom', 'whose', 'if', 'unless', 'until', 'while', 'since', 'as', 'so', 'than', 'such',
        'both', 'either', 'neither', 'not', 'no', 'nor', 'only', 'own', 'same', 'few', 'more',
        'most', 'other', 'some', 'any', 'each', 'every', 'all', 'both', 'half', 'either',
        'neither', 'many', 'much', 'several', 'enough', 'too', 'very', 'quite', 'rather',
        'just', 'even', 'still', 'yet', 'already', 'also', 'again', 'once', 'twice', 'here',
        'there', 'everywhere', 'anywhere', 'somewhere', 'nowhere', 'today', 'yesterday',
        'tomorrow', 'now', 'then', 'soon', 'later', 'early', 'late', 'always', 'never',
        'often', 'sometimes', 'usually', 'seldom', 'rarely', 'hardly', 'nearly', 'almost',
        'quite', 'completely', 'entirely', 'totally', 'absolutely', 'perfectly', 'exactly',
        'probably', 'perhaps', 'maybe', 'certainly', 'definitely', 'surely', 'clearly',
        'obviously', 'apparently', 'generally', 'particularly', 'especially', 'specifically',
        'mainly', 'mostly', 'largely', 'partly', 'slightly', 'somewhat', 'rather', 'fairly',
        'pretty', 'really', 'actually', 'indeed', 'truly', 'seriously', 'honestly', 'frankly',
        'basically', 'essentially', 'fundamentally', 'originally', 'initially', 'finally',
        'eventually', 'ultimately', 'consequently', 'therefore', 'thus', 'hence', 'accordingly',
        'however', 'nevertheless', 'nonetheless', 'otherwise', 'meanwhile', 'furthermore',
        'moreover', 'additionally', 'besides', 'also', 'too', 'as', 'well', 'including',
        'such', 'like', 'unlike', 'similar', 'different', 'same', 'equal', 'equivalent',
        'comparable', 'related', 'relevant', 'important', 'significant', 'major', 'minor',
        'primary', 'secondary', 'main', 'key', 'central', 'basic', 'fundamental', 'essential',
        'necessary', 'required', 'optional', 'possible', 'impossible', 'likely', 'unlikely',
        'certain', 'uncertain', 'sure', 'unsure', 'clear', 'unclear', 'obvious', 'hidden',
        'known', 'unknown', 'public', 'private', 'open', 'closed', 'free', 'paid', 'cheap',
        'expensive', 'high', 'low', 'big', 'small', 'large', 'little', 'huge', 'tiny',
        'long', 'short', 'wide', 'narrow', 'thick', 'thin', 'heavy', 'light', 'strong',
        'weak', 'hard', 'soft', 'rough', 'smooth', 'hot', 'cold', 'warm', 'cool', 'dry',
        'wet', 'clean', 'dirty', 'new', 'old', 'young', 'fresh', 'stale', 'good', 'bad',
        'best', 'worst', 'better', 'worse', 'great', 'terrible', 'excellent', 'poor',
        'perfect', 'awful', 'wonderful', 'horrible', 'amazing', 'boring', 'interesting',
        'exciting', 'dull', 'fun', 'serious', 'funny', 'sad', 'happy', 'angry', 'calm',
        'quiet', 'loud', 'fast', 'slow', 'quick', 'gradual', 'sudden', 'immediate',
        'instant', 'delayed', 'early', 'late', 'recent', 'ancient', 'modern', 'traditional',
        'conventional', 'innovative', 'creative', 'original', 'unique', 'common', 'rare',
        'usual', 'unusual', 'normal', 'abnormal', 'regular', 'irregular', 'standard',
        'special', 'general', 'specific', 'particular', 'individual', 'personal', 'social',
        'political', 'economic', 'financial', 'commercial', 'industrial', 'agricultural',
        'educational', 'medical', 'legal', 'technical', 'scientific', 'artistic', 'cultural',
        'historical', 'geographical', 'physical', 'mental', 'emotional', 'spiritual',
        'natural', 'artificial', 'real', 'fake', 'true', 'false', 'right', 'wrong',
        'correct', 'incorrect', 'accurate', 'inaccurate', 'precise', 'vague', 'exact',
        'approximate', 'complete', 'incomplete', 'full', 'empty', 'total', 'partial',
        'whole', 'half', 'quarter', 'third', 'double', 'triple', 'single', 'multiple',
        'first', 'second', 'third', 'last', 'next', 'previous', 'following', 'preceding',
        'above', 'below', 'over', 'under', 'inside', 'outside', 'within', 'without',
        'near', 'far', 'close', 'distant', 'local', 'foreign', 'domestic', 'international',
        'global', 'worldwide', 'universal', 'regional', 'national', 'state', 'federal',
        'government', 'official', 'unofficial', 'formal', 'informal', 'legal', 'illegal',
        'valid', 'invalid', 'effective', 'ineffective', 'successful', 'unsuccessful',
        'positive', 'negative', 'active', 'passive', 'direct', 'indirect', 'forward',
        'backward', 'upward', 'downward', 'inward', 'outward', 'toward', 'away', 'across',
        'along', 'around', 'behind', 'beside', 'beyond', 'despite', 'except', 'including',
        'excluding', 'regarding', 'concerning', 'considering', 'given', 'provided',
        'assuming', 'suppose', 'imagine', 'believe', 'think', 'know', 'understand',
        'realize', 'recognize', 'remember', 'forget', 'learn', 'teach', 'study', 'research',
        'investigate', 'explore', 'discover', 'find', 'search', 'look', 'see', 'watch',
        'observe', 'notice', 'hear', 'listen', 'speak', 'talk', 'say', 'tell', 'ask',
        'answer', 'reply', 'respond', 'explain', 'describe', 'discuss', 'mention',
        'suggest', 'recommend', 'advise', 'warn', 'inform', 'notify', 'announce',
        'declare', 'state', 'claim', 'argue', 'prove', 'demonstrate', 'show', 'reveal',
        'hide', 'cover', 'protect', 'defend', 'attack', 'fight', 'win', 'lose', 'beat',
        'defeat', 'succeed', 'fail', 'try', 'attempt', 'effort', 'work', 'job', 'task',
        'duty', 'responsibility', 'role', 'function', 'purpose', 'goal', 'aim', 'target',
        'objective', 'plan', 'strategy', 'method', 'way', 'means', 'approach', 'technique',
        'process', 'procedure', 'system', 'structure', 'organization', 'institution',
        'company', 'business', 'industry', 'market', 'economy', 'finance', 'money',
        'cost', 'price', 'value', 'worth', 'benefit', 'advantage', 'disadvantage',
        'problem', 'issue', 'challenge', 'difficulty', 'trouble', 'risk', 'danger',
        'threat', 'opportunity', 'chance', 'possibility', 'option', 'choice', 'decision',
        'conclusion', 'result', 'outcome', 'consequence', 'effect', 'impact', 'influence',
        'change', 'development', 'improvement', 'progress', 'growth', 'increase', 'decrease',
        'reduction', 'decline', 'rise', 'fall', 'gain', 'loss', 'profit', 'revenue',
        'income', 'expense', 'investment', 'return', 'interest', 'rate', 'percentage',
        'ratio', 'proportion', 'amount', 'quantity', 'number', 'figure', 'data',
        'information', 'knowledge', 'fact', 'detail', 'feature', 'characteristic',
        'quality', 'property', 'attribute', 'aspect', 'element', 'component', 'part',
        'section', 'area', 'region', 'zone', 'location', 'place', 'position', 'site',
        'spot', 'point', 'line', 'curve', 'circle', 'square', 'triangle', 'shape',
        'form', 'size', 'dimension', 'length', 'width', 'height', 'depth', 'distance',
        'space', 'room', 'capacity', 'volume', 'weight', 'mass', 'density', 'pressure',
        'temperature', 'degree', 'level', 'grade', 'rank', 'status', 'condition',
        'situation', 'circumstance', 'context', 'environment', 'setting', 'background',
        'history', 'past', 'present', 'future', 'time', 'period', 'moment', 'instant',
        'second', 'minute', 'hour', 'day', 'week', 'month', 'year', 'decade', 'century',
        'age', 'generation', 'era', 'epoch', 'season', 'spring', 'summer', 'autumn',
        'winter', 'morning', 'afternoon', 'evening', 'night', 'midnight', 'noon',
        'dawn', 'dusk', 'sunrise', 'sunset', 'monday', 'tuesday', 'wednesday', 'thursday',
        'friday', 'saturday', 'sunday', 'january', 'february', 'march', 'april', 'may',
        'june', 'july', 'august', 'september', 'october', 'november', 'december'
    }
    
    # ê¸°ìˆ /ë¹„ì¦ˆë‹ˆìŠ¤ ê´€ë ¨ ì¤‘ìš” í‚¤ì›Œë“œëŠ” ë³´ì¡´
    important_keywords = {
        'ai', 'ml', 'api', 'cloud', 'data', 'tech', 'software', 'hardware', 'system',
        'platform', 'service', 'solution', 'product', 'company', 'business', 'market',
        'revenue', 'growth', 'investment', 'funding', 'startup', 'enterprise', 'digital',
        'innovation', 'technology', 'development', 'research', 'analysis', 'security',
        'privacy', 'performance', 'optimization', 'automation', 'integration', 'deployment',
        'infrastructure', 'architecture', 'framework', 'database', 'analytics', 'monitoring',
        'observability', 'devops', 'kubernetes', 'docker', 'microservices', 'serverless',
        'machine', 'learning', 'artificial', 'intelligence', 'neural', 'network', 'algorithm',
        'model', 'training', 'prediction', 'classification', 'regression', 'clustering',
        'visualization', 'dashboard', 'metrics', 'logs', 'traces', 'alerts', 'incidents'
    }
    
    filtered_words = []
    for word in words:
        if (len(word) >= min_length and 
            word not in english_stopwords and 
            not word.isdigit() and
            re.match(r'^[a-zA-Z]+$', word)):  # ì˜ì–´ ë‹¨ì–´ë§Œ
            # ì¤‘ìš” í‚¤ì›Œë“œì´ê±°ë‚˜ ëŒ€ë¬¸ìë¡œ ì‹œì‘í•˜ëŠ” ë‹¨ì–´(ê³ ìœ ëª…ì‚¬ ê°€ëŠ¥ì„±)ëŠ” í¬í•¨
            if word in important_keywords or word[0].isupper() or len(word) >= 4:
                filtered_words.append(word.title())  # ì²« ê¸€ì ëŒ€ë¬¸ìë¡œ í†µì¼
    
    return filtered_words

def extract_keywords_konlpy(text: str) -> List[str]:
    """KoNLPyë¥¼ ì‚¬ìš©í•œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
    if not text:
        return []
    
    try:
        # ëª…ì‚¬ì™€ ê³ ìœ ëª…ì‚¬ë§Œ ì¶”ì¶œ
        nouns = okt.nouns(text)
        # ê¸¸ì´ 2 ì´ìƒì¸ ë‹¨ì–´ë§Œ í•„í„°ë§
        filtered_nouns = [noun for noun in nouns if len(noun) >= 2 and not noun.isdigit()]
        return filtered_nouns
    except Exception as e:
        print(f"KoNLPy ë¶„ì„ ì˜¤ë¥˜: {e}")
        return extract_keywords_simple(text)

def extract_keywords(text: str) -> List[str]:
    """í‚¤ì›Œë“œ ì¶”ì¶œ (KoNLPy ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ ì‚¬ìš©, ì•„ë‹ˆë©´ ê°„ë‹¨í•œ ë°©ë²•)"""
    if KONLPY_AVAILABLE:
        return extract_keywords_konlpy(text)
    else:
        return extract_keywords_simple(text)

def is_korean_text(text: str) -> bool:
    """í…ìŠ¤íŠ¸ê°€ í•œêµ­ì–´ì¸ì§€ í™•ì¸"""
    if not text:
        return False
    korean_chars = len(re.findall(r'[ê°€-í£]', text))
    total_chars = len(re.findall(r'[ê°€-í£a-zA-Z]', text))
    return korean_chars > total_chars * 0.3 if total_chars > 0 else False

def process_article_keywords(article: Dict) -> List[str]:
    """ë‹¨ì¼ ê¸°ì‚¬ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ (í•œêµ­ì–´/ì˜ì–´ êµ¬ë¶„)"""
    all_keywords = []
    
    # ì œëª©ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ (ì˜ì–´)
    title = article.get('title', '')
    if title:
        if is_korean_text(title):
            title_keywords = extract_keywords(title)  # í•œêµ­ì–´ ì²˜ë¦¬
        else:
            title_keywords = extract_english_keywords(title)  # ì˜ì–´ ì²˜ë¦¬
        all_keywords.extend(title_keywords)
    
    # í•œêµ­ì–´ ìš”ì•½ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
    summary_ko = article.get('summary_ko', '')
    if summary_ko:
        summary_keywords = extract_keywords(summary_ko)
        all_keywords.extend(summary_keywords)
    
    # í•œêµ­ì–´ ì˜í–¥ ë¶„ì„ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
    impact_ko = article.get('impact_ko', '')
    if impact_ko:
        impact_keywords = extract_keywords(impact_ko)
        all_keywords.extend(impact_keywords)
    
    # ì›ë³¸ summaryì—ì„œ ì˜ì–´ í‚¤ì›Œë“œ ì¶”ì¶œ
    original_summary = article.get('summary', '')
    if original_summary and not is_korean_text(original_summary):
        original_summary_keywords = extract_english_keywords(original_summary)
        all_keywords.extend(original_summary_keywords)
    
    # ì¤‘ë³µ ì œê±° ë° í•„í„°ë§
    unique_keywords = list(set([k for k in all_keywords if len(k) >= 2]))
    
    return unique_keywords

def update_keyword_data():
    """ëª¨ë“  ë‰´ìŠ¤ ë°ì´í„°ì—ì„œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ì—¬ keywords collection ì—…ë°ì´íŠ¸"""
    try:
        print("ğŸ”„ í‚¤ì›Œë“œ ë°ì´í„° ì—…ë°ì´íŠ¸ ì‹œì‘...")
        
        # ëª¨ë“  ë‰´ìŠ¤ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        news_data = list(news_collection.find({}))
        
        if not news_data:
            print("âš ï¸ ë‰´ìŠ¤ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # í‚¤ì›Œë“œ ë¹ˆë„ ê³„ì‚°
        keyword_frequency = Counter()
        keyword_articles = defaultdict(list)  # í‚¤ì›Œë“œë³„ ê¸°ì‚¬ ID ì €ì¥
        keyword_cooccurrence = defaultdict(int)  # í‚¤ì›Œë“œ ë™ì‹œ ì¶œí˜„
        daily_keywords = defaultdict(lambda: defaultdict(int))  # ë‚ ì§œë³„ í‚¤ì›Œë“œ ë¹ˆë„
        
        for article in news_data:
            article_id = article.get('_id')
            article_keywords = process_article_keywords(article)
            
            if not article_keywords:
                continue
            
            # ê¸°ì‚¬ ë°œí–‰ì¼ íŒŒì‹±
            try:
                if isinstance(article.get('published'), str):
                    pub_date = datetime.fromisoformat(article['published'].replace('Z', '+00:00')).date()
                else:
                    pub_date = article.get('published', datetime.utcnow()).date()
            except:
                pub_date = datetime.utcnow().date()
            
            # í‚¤ì›Œë“œ ë¹ˆë„ ë° ê¸°ì‚¬ ì—°ê²° ì •ë³´ ì—…ë°ì´íŠ¸
            for keyword in article_keywords:
                keyword_frequency[keyword] += 1
                keyword_articles[keyword].append({
                    'article_id': article_id,
                    'title': article.get('title', ''),
                    'published': pub_date.isoformat(),
                    'source': article.get('source', '')
                })
                daily_keywords[pub_date][keyword] += 1
            
            # í‚¤ì›Œë“œ ë™ì‹œ ì¶œí˜„ ê³„ì‚°
            for i, keyword1 in enumerate(article_keywords):
                for keyword2 in article_keywords[i+1:]:
                    pair = tuple(sorted([keyword1, keyword2]))
                    keyword_cooccurrence[pair] += 1
        
        # keywords collection ì—…ë°ì´íŠ¸
        keywords_operations = []
        for keyword, frequency in keyword_frequency.items():
            keywords_operations.append(UpdateOne(
                {'keyword': keyword},
                {
                    '$set': {
                        'keyword': keyword,
                        'frequency': frequency,
                        'articles': keyword_articles[keyword],
                        'last_updated': datetime.utcnow()
                    }
                },
                upsert=True
            ))
        
        if keywords_operations:
            keywords_collection.bulk_write(keywords_operations)
            print(f"âœ… {len(keywords_operations)}ê°œ í‚¤ì›Œë“œ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
        
        # keyword_trends collection ì—…ë°ì´íŠ¸
        trends_operations = []
        for date, keywords in daily_keywords.items():
            for keyword, count in keywords.items():
                trends_operations.append(UpdateOne(
                    {'keyword': keyword, 'date': date.isoformat()},
                    {
                        '$set': {
                            'keyword': keyword,
                            'date': date.isoformat(),
                            'count': count,
                            'last_updated': datetime.utcnow()
                        }
                    },
                    upsert=True
                ))
        
        if trends_operations:
            keyword_trends_collection.bulk_write(trends_operations)
            print(f"âœ… {len(trends_operations)}ê°œ íŠ¸ë Œë“œ ë°ì´í„° ì—…ë°ì´íŠ¸ ì™„ë£Œ")
        
        # í‚¤ì›Œë“œ ë™ì‹œ ì¶œí˜„ ë°ì´í„° ì €ì¥
        cooccurrence_operations = []
        cooccurrence_collection = client["bitsfeed"]["keyword_cooccurrence"]
        
        for (keyword1, keyword2), count in keyword_cooccurrence.items():
            cooccurrence_operations.append(UpdateOne(
                {'keyword1': keyword1, 'keyword2': keyword2},
                {
                    '$set': {
                        'keyword1': keyword1,
                        'keyword2': keyword2,
                        'count': count,
                        'last_updated': datetime.utcnow()
                    }
                },
                upsert=True
            ))
        
        if cooccurrence_operations:
            cooccurrence_collection.bulk_write(cooccurrence_operations)
            print(f"âœ… {len(cooccurrence_operations)}ê°œ ë™ì‹œì¶œí˜„ ë°ì´í„° ì—…ë°ì´íŠ¸ ì™„ë£Œ")
        
        print("ğŸ‰ í‚¤ì›Œë“œ ë°ì´í„° ì—…ë°ì´íŠ¸ ì™„ë£Œ!")
        
        return {
            'total_articles': len(news_data),
            'total_keywords': len(keyword_frequency),
            'total_cooccurrences': len(keyword_cooccurrence)
        }
        
    except Exception as e:
        print(f"âŒ í‚¤ì›Œë“œ ë°ì´í„° ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
        raise e

def get_keyword_frequency(days: Optional[int] = 7, limit: Optional[int] = 50) -> Dict:
    """í‚¤ì›Œë“œ ë¹ˆë„ ë°ì´í„° ì¡°íšŒ (MongoDBì—ì„œ)"""
    try:
        # ë‚ ì§œ í•„í„°ë§ì„ ìœ„í•œ pipeline
        pipeline = []
        
        if days:
            cutoff_date = (datetime.now() - timedelta(days=days)).date().isoformat()
            pipeline.append({
                '$match': {
                    'articles.published': {'$gte': cutoff_date}
                }
            })
        
        pipeline.extend([
            {'$sort': {'frequency': -1}},
            {'$limit': limit}
        ])
        
        results = list(keywords_collection.aggregate(pipeline))
        
        top_keywords = {item['keyword']: item['frequency'] for item in results}
        
        return {
            'total_keywords': keywords_collection.count_documents({}),
            'top_keywords': top_keywords,
            'analysis_period_days': days
        }
        
    except Exception as e:
        print(f"âŒ í‚¤ì›Œë“œ ë¹ˆë„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return {'error': str(e)}

def get_keyword_trends(days: Optional[int] = 30, top_n: Optional[int] = 10) -> Dict:
    """í‚¤ì›Œë“œ íŠ¸ë Œë“œ ë°ì´í„° ì¡°íšŒ (MongoDBì—ì„œ)"""
    try:
        cutoff_date = (datetime.now() - timedelta(days=days)).date()
        
        # ìƒìœ„ í‚¤ì›Œë“œ ì„ íƒ
        top_keywords = list(keywords_collection.find({}, {'keyword': 1, 'frequency': 1})
                           .sort('frequency', -1).limit(top_n))
        top_keyword_list = [item['keyword'] for item in top_keywords]
        
        # ë‚ ì§œë³„ íŠ¸ë Œë“œ ë°ì´í„° ì¡°íšŒ
        pipeline = [
            {
                '$match': {
                    'keyword': {'$in': top_keyword_list},
                    'date': {'$gte': cutoff_date.isoformat()}
                }
            },
            {
                '$group': {
                    '_id': {'keyword': '$keyword', 'date': '$date'},
                    'count': {'$sum': '$count'}
                }
            },
            {
                '$sort': {'_id.date': 1}
            }
        ]
        
        results = list(keyword_trends_collection.aggregate(pipeline))
        
        # ë°ì´í„° êµ¬ì¡°í™”
        trend_data = defaultdict(dict)
        dates = set()
        
        for item in results:
            keyword = item['_id']['keyword']
            date = item['_id']['date']
            count = item['count']
            trend_data[keyword][date] = count
            dates.add(date)
        
        dates = sorted(dates)
        
        # ëˆ„ë½ëœ ë‚ ì§œëŠ” 0ìœ¼ë¡œ ì±„ì›€
        final_trend_data = {}
        for keyword in top_keyword_list:
            daily_counts = []
            for date in dates:
                count = trend_data[keyword].get(date, 0)
                daily_counts.append(count)
            final_trend_data[keyword] = daily_counts
        
        return {
            'trend_data': final_trend_data,
            'dates': dates,
            'top_keywords': top_keyword_list,
            'analysis_days': len(dates)
        }
        
    except Exception as e:
        print(f"âŒ í‚¤ì›Œë“œ íŠ¸ë Œë“œ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return {'error': str(e)}

def get_keyword_network(days: Optional[int] = 7, min_cooccurrence: Optional[int] = 2) -> Dict:
    """í‚¤ì›Œë“œ ë„¤íŠ¸ì›Œí¬ ë°ì´í„° ì¡°íšŒ (MongoDBì—ì„œ)"""
    try:
        cooccurrence_collection = client["bitsfeed"]["keyword_cooccurrence"]
        
        # ë™ì‹œ ì¶œí˜„ ë°ì´í„° ì¡°íšŒ
        query = {'count': {'$gte': min_cooccurrence}}
        cooccurrences = list(cooccurrence_collection.find(query))
        
        if not cooccurrences:
            return {'error': f'ìµœì†Œ {min_cooccurrence}íšŒ ì´ìƒ í•¨ê»˜ ë‚˜íƒ€ë‚˜ëŠ” í‚¤ì›Œë“œ ìŒì´ ì—†ìŠµë‹ˆë‹¤.'}
        
        # ë„¤íŠ¸ì›Œí¬ ë°ì´í„° êµ¬ì„±
        nodes = set()
        edges = []
        
        for item in cooccurrences:
            keyword1 = item['keyword1']
            keyword2 = item['keyword2']
            count = item['count']
            
            nodes.add(keyword1)
            nodes.add(keyword2)
            edges.append({
                'source': keyword1,
                'target': keyword2,
                'weight': count
            })
        
        # ë…¸ë“œë³„ ì—°ê²° ê°•ë„ ê³„ì‚°
        node_strength = defaultdict(int)
        for edge in edges:
            node_strength[edge['source']] += edge['weight']
            node_strength[edge['target']] += edge['weight']
        
        nodes_data = [
            {'id': node, 'strength': node_strength[node]}
            for node in nodes
        ]
        
        return {
            'nodes': nodes_data,
            'edges': edges,
            'total_nodes': len(nodes),
            'total_edges': len(edges),
            'min_cooccurrence': min_cooccurrence
        }
        
    except Exception as e:
        print(f"âŒ í‚¤ì›Œë“œ ë„¤íŠ¸ì›Œí¬ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return {'error': str(e)}

def initialize_keyword_system():
    """í‚¤ì›Œë“œ ì‹œìŠ¤í…œ ì´ˆê¸°í™” (ì¸ë±ìŠ¤ ìƒì„± + ë°ì´í„° ì—…ë°ì´íŠ¸)"""
    print("ğŸš€ í‚¤ì›Œë“œ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹œì‘...")
    create_indexes()
    return update_keyword_data()

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    initialize_keyword_system() 